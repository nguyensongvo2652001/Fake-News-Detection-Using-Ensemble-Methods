{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "FakeNewsDetection(NoLinguistic)(VotingClassifier).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_AOond6EEih",
        "outputId": "c12a12ff-0eba-4bd9-a1b8-ef1eda3c19dc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4mg7RXeEQTy"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIoGxoFsETFQ",
        "outputId": "f2ed349c-e50d-448e-8cbc-69d8b7da8c28"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJAmL0_7EVWe"
      },
      "source": [
        "def clean_text(text):\n",
        "    if text:\n",
        "        text = f'{text}'\n",
        "        text = text.lower()\n",
        "        text = text.split()\n",
        "        ps = PorterStemmer()\n",
        "        text = [ps.stem(word) for word in text if text not in stopwords.words('english')]\n",
        "        text = ' '.join(text)\n",
        "        return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhyQPJqaEXol"
      },
      "source": [
        "#Import dataset 5 (185 samples)\n",
        "X  = []\n",
        "y = []\n",
        "\n",
        "with open(\"/content/gdrive/MyDrive/special_fake_news.txt\",'r',encoding = 'latin-1') as file:\n",
        "    texts = file.read().split('\\n')\n",
        "    for text in texts:\n",
        "        if text:\n",
        "            X.append(clean_text(text))\n",
        "            y.append(0)\n",
        "\n",
        "with open(\"/content/gdrive/MyDrive/special_real_news.txt\",'r',encoding = 'latin-1') as file:\n",
        "    texts = file.read().split('\\n')\n",
        "    for text in texts:\n",
        "        if text:\n",
        "            X.append(clean_text(text))\n",
        "            y.append(1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8Ms8KKdBtHP"
      },
      "source": [
        "#Importing dataset 1 (First 5000 samples)\n",
        "data = pd.read_csv(\"/content/gdrive/MyDrive/train.csv\")\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "for i in range(5000):\n",
        "  X.append(clean_text(data['title'][i]))\n",
        "  y.append(data['label'][i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_cNN4RInGDg"
      },
      "source": [
        "#Import dataset 2 (First 5000 samples)\n",
        "data = pd.read_csv(\"/content/gdrive/MyDrive/fake_or_real_news.csv\")\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in range(5000):\n",
        "    X.append(clean_text(data['title'][i]))\n",
        "    if data['label'][i] == 'FAKE':\n",
        "        y.append(0)\n",
        "    else:\n",
        "        y.append(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9Zl59I4zDJC"
      },
      "source": [
        "#Import dataset 3\n",
        "fake_data = pd.read_csv(\"/content/gdrive/MyDrive/Fake.csv\",engine = 'python')\n",
        "true_data = pd.read_csv(\"/content/gdrive/MyDrive/True.csv\",engine = 'python')\n",
        "\n",
        "#Take 2500 samples from true data , 4064 samples from fake data\n",
        "X = []\n",
        "y = []\n",
        "for i in range(4064):\n",
        "    X.append(clean_text(fake_data['title'][i]))\n",
        "    y.append(0)\n",
        "for i in range(2500):\n",
        "    X.append(clean_text(true_data['title'][i]))\n",
        "    y.append(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAR1lgafAWzY"
      },
      "source": [
        "#Import dataset 4 (7950 samples)\n",
        "import sys\n",
        "import csv\n",
        "\n",
        "csv.field_size_limit(sys.maxsize)\n",
        "fake_data = pd.read_csv(\"/content/gdrive/MyDrive/Fake.csv\",engine = 'python')\n",
        "fake_data_2 = pd.read_csv(\"/content/gdrive/MyDrive/fake2.csv\",engine = 'python')\n",
        "true_data = pd.read_csv(\"/content/gdrive/MyDrive/True.csv\",engine = 'python')\n",
        "\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "for i in range(1620):\n",
        "    X.append(clean_text(fake_data['title'][i]))\n",
        "    y.append(0)\n",
        "for i in range(2880):\n",
        "    X.append(clean_text(fake_data_2['title'][i]))\n",
        "    y.append(0)\n",
        "for i in range(3450):\n",
        "    X.append(clean_text(true_data['title'][i]))\n",
        "    y.append(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV8ksU5OmMMO"
      },
      "source": [
        "#Import dataset 6 \n",
        "data = pd.read_csv(\"/content/gdrive/MyDrive/data.csv\")\n",
        "X = []\n",
        "y = data['Label']\n",
        "for i in range(len(data)):\n",
        "    X.append(clean_text(data['Headline'][i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdfHny7FrFdZ"
      },
      "source": [
        "#Import dataset 7 \n",
        "X = []\n",
        "y = []\n",
        "\n",
        "#Get 2000 samples from dataset 6 \n",
        "data = pd.read_csv(\"/content/gdrive/MyDrive/data.csv\")\n",
        "for i in range(2000):\n",
        "    X.append(clean_text(data['Headline'][i]))\n",
        "    y.append(data['Label'][i])\n",
        "\n",
        "#Get all from dataset 5\n",
        "with open(\"/content/gdrive/MyDrive/special_fake_news.txt\",'r',encoding = 'latin-1') as file:\n",
        "    texts = file.read().split('\\n')\n",
        "    for text in texts:\n",
        "        if text:\n",
        "            X.append(clean_text(text))\n",
        "            y.append(0)\n",
        "\n",
        "with open(\"/content/gdrive/MyDrive/special_real_news.txt\",'r',encoding = 'latin-1') as file:\n",
        "    texts = file.read().split('\\n')\n",
        "    for text in texts:\n",
        "        if text:\n",
        "            X.append(clean_text(text))\n",
        "            y.append(1)\n",
        "\n",
        "#Get 2000 samples from dataset 1\n",
        "data = pd.read_csv(\"/content/gdrive/MyDrive/train.csv\")\n",
        "for i in range(5000):\n",
        "  X.append(clean_text(data['title'][i]))\n",
        "  y.append(data['label'][i])\n",
        "\n",
        "#Get 3000 samples from dataset 3\n",
        "fake_data = pd.read_csv(\"/content/gdrive/MyDrive/Fake.csv\",engine = 'python')\n",
        "true_data = pd.read_csv(\"/content/gdrive/MyDrive/True.csv\",engine = 'python')\n",
        "\n",
        "#Take 1100 samples from true data , 1900 samples from fake data\n",
        "for i in range(1100):\n",
        "    X.append(clean_text(fake_data['title'][i]))\n",
        "    y.append(0)\n",
        "for i in range(1900):\n",
        "    X.append(clean_text(true_data['title'][i]))\n",
        "    y.append(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zqUoj9tEhpQ"
      },
      "source": [
        "#Features selection\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "X = cv.fit_transform(X).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoXWDcx9E6KV"
      },
      "source": [
        "#Split test set\n",
        "from sklearn.model_selection import train_test_split \n",
        "X_train , X_test , y_train , y_test = train_test_split(X,y, test_size = 0.30,random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAmcSpwoE73J"
      },
      "source": [
        "#Import necessary libraries for classification\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import VotingClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRV_80Y_F8xk"
      },
      "source": [
        "#Build and fit model to dataset\n",
        "classifier = VotingClassifier(estimators = [('naive_bayes',MultinomialNB()),\n",
        "                                            ('logistic_regression',LogisticRegression(n_jobs = -1 , random_state = 0)),\n",
        "                                            #('knn',KNeighborsClassifier(n_neighbors=10,n_jobs = -1)),\n",
        "                                            ('svm',SVC(random_state = 0))\n",
        "                                            ],\n",
        "                                            voting = 'hard',\n",
        "                                            n_jobs = -1)\n",
        "\n",
        "classifier = classifier.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMPRCb3ank-L"
      },
      "source": [
        "y_pred = classifier.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZvapGGCoFj7"
      },
      "source": [
        "#Making confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test , y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm07UhSqoIro"
      },
      "source": [
        "def calculate_accuracy(cm):\n",
        "    tn = cm[0][0]\n",
        "    tp = cm[1][1]\n",
        "    fp = cm[0][1]\n",
        "    fn = cm[1][0]\n",
        "        \n",
        "    accuracy = (tn + tp) / (tn + tp + fp + fn)\n",
        "    precision = tp/(tp+fp)\n",
        "    recall = tp/(tp+fn)\n",
        "    f1 = 2*(precision*recall)/(precision + recall)\n",
        "    average = (accuracy + precision + recall + f1) /4 \n",
        "    return [accuracy , precision , recall , f1 , average]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzVlJDYzoKBg"
      },
      "source": [
        "acc = calculate_accuracy(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jm52Lp9soLWo",
        "outputId": "70d1fd6d-4a5a-4bbf-fec2-575682a20adc"
      },
      "source": [
        "acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8459512837393022,\n",
              " 0.8142292490118577,\n",
              " 0.9120809614168248,\n",
              " 0.8603818615751789,\n",
              " 0.858160838935791]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3x5XYDU-mpnm",
        "outputId": "14891b19-51d8-45e7-abac-c02cd0421a42"
      },
      "source": [
        "len(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10125"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    }
  ]
}