{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FakeNewsDetection(Linguistic)(VotingClassifier).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGdnFkrQo5Bq",
        "outputId": "248fa450-65a4-47ef-c638-8fd3cf725af1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRxxFYpzpE3i"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TbyxADipIO3",
        "outputId": "d16360e4-674e-4b9f-a3fc-4dc86749ced1"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import cmudict\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('tagsets')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "nltk_tags = [\"''\",'(',')',',','--','.',':','CC','CD','DT','EX','FW','IN','JJ','JJR','JJS','LS','MD'\n",
        "             ,'NN','NNP','NNPS','NNS','PDT','POS','PRP','PRP$','RB','RBR','RBS','RP','SYM','TO','UH',\n",
        "             'VB','VBD','VBG','VBN','VBP','VBZ','WDT','WP','WP$','WRB']\n",
        "from nltk.stem.porter import PorterStemmer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5PlcDGTpLie"
      },
      "source": [
        "def clean_text(text):\n",
        "    if text:\n",
        "        text = f'{text}'\n",
        "        #text = text.lower()\n",
        "        text = text.split()\n",
        "        ps = PorterStemmer()\n",
        "        text = [ps.stem(word) for word in text if text not in stopwords.words('english')]\n",
        "        text = ' '.join(text)\n",
        "        return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo4f-dx_pO4g"
      },
      "source": [
        "def count_uppercase(text):\n",
        "    result = 0\n",
        "    for char in text:\n",
        "        if char.isupper():\n",
        "            result += 1 \n",
        "            \n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZghT4i3pQV5"
      },
      "source": [
        "def count_stopwords(text):\n",
        "    cnt = 0\n",
        "    for word in text.split(' '):\n",
        "        if word in stopwords.words('english'):\n",
        "            cnt += 1\n",
        "            \n",
        "    return cnt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrV-JdCwpSDD"
      },
      "source": [
        "def tag_text(text):\n",
        "    text = nltk.word_tokenize(text)\n",
        "    return nltk.pos_tag(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aIN44urpUPT"
      },
      "source": [
        "def feature_selection_text(text):\n",
        "    import textstat\n",
        "    result = []\n",
        "    #Syntax-based features \n",
        "    result.append(len(text)) #count chars\n",
        "    result.append(len(text.split(' '))) #count words\n",
        "    result.append(count_stopwords(text)) #count stopwords\n",
        "    result.append(count_uppercase(text)) #Count uppercase\n",
        "\n",
        "    \n",
        "    #Grammatical evidence\n",
        "    #Count tags\n",
        "    tags_dict = {}\n",
        "    for tag in nltk_tags:\n",
        "        tags_dict[tag] = 0\n",
        "    \n",
        "    tags = tag_text(text)\n",
        "    for _,tag in tags:\n",
        "        try:\n",
        "            tags_dict[tag] += 1\n",
        "        except:\n",
        "            continue\n",
        "    \n",
        "    for i in range(len(nltk_tags)):\n",
        "        nltk_tag = nltk_tags[i]\n",
        "        result.append(tags_dict[nltk_tag])\n",
        "    \n",
        "\n",
        "    \n",
        "    #Readability features \n",
        "    result.append(textstat.flesch_reading_ease(text))\n",
        "    result.append(textstat.smog_index(text))\n",
        "    result.append(textstat.flesch_kincaid_grade(text))\n",
        "    result.append(textstat.coleman_liau_index(text))\n",
        "    result.append(textstat.automated_readability_index(text))\n",
        "    result.append(textstat.linsear_write_formula(text))\n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4dKyHY9pVmI"
      },
      "source": [
        "def feature_selection_texts(arr):\n",
        "    return np.array([feature_selection_text(item) for item in arr])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ghjewu_ipZ6I"
      },
      "source": [
        "#Import dataset 5 (185 samples)\n",
        "X  = []\n",
        "y = []\n",
        "\n",
        "with open(\"/content/gdrive/MyDrive/special_fake_news.txt\",'r',encoding = 'latin-1') as file:\n",
        "    texts = file.read().split('\\n')\n",
        "    for text in texts:\n",
        "        if text:\n",
        "            X.append(text)\n",
        "            y.append(0)\n",
        "\n",
        "with open(\"/content/gdrive/MyDrive/special_real_news.txt\",'r',encoding = 'latin-1') as file:\n",
        "    texts = file.read().split('\\n')\n",
        "    for text in texts:\n",
        "        if text:\n",
        "            X.append(text)\n",
        "            y.append(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALzo-tjHEa6T"
      },
      "source": [
        "#Importing dataset 1 (First 5000 samples)\n",
        "data = pd.read_csv(\"/content/gdrive/MyDrive/train.csv\")\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "for i in range(5000):\n",
        "  X.append(clean_text(data['title'][i]))\n",
        "  y.append(data['label'][i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMJn2Mv_GlcK"
      },
      "source": [
        "#Import dataset 2 (First 5000 samples)\n",
        "data = pd.read_csv(\"/content/gdrive/MyDrive/fake_or_real_news.csv\")\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in range(5000):\n",
        "    X.append(clean_text(data['title'][i]))\n",
        "    if data['label'][i] == 'FAKE':\n",
        "        y.append(0)\n",
        "    else:\n",
        "        y.append(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3HH3hzx04eE"
      },
      "source": [
        "#Import dataset 3\n",
        "fake_data = pd.read_csv(\"/content/gdrive/MyDrive/Fake.csv\",engine = 'python')\n",
        "true_data = pd.read_csv(\"/content/gdrive/MyDrive/True.csv\",engine = 'python')\n",
        "\n",
        "#Take 2500 samples from true data , 4064 samples from fake data\n",
        "X = []\n",
        "y = []\n",
        "for i in range(4064):\n",
        "    X.append(clean_text(fake_data['title'][i]))\n",
        "    y.append(0)\n",
        "for i in range(2500):\n",
        "    X.append(clean_text(true_data['title'][i]))\n",
        "    y.append(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0F8qimnBcOy"
      },
      "source": [
        "#Import dataset 4 (7950 samples)\n",
        "import sys\n",
        "import csv\n",
        "\n",
        "csv.field_size_limit(sys.maxsize)\n",
        "fake_data = pd.read_csv(\"/content/gdrive/MyDrive/Fake.csv\",engine = 'python')\n",
        "fake_data_2 = pd.read_csv(\"/content/gdrive/MyDrive/fake2.csv\",engine = 'python')\n",
        "true_data = pd.read_csv(\"/content/gdrive/MyDrive/True.csv\",engine = 'python')\n",
        "\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "for i in range(1620):\n",
        "    X.append(clean_text(fake_data['title'][i]))\n",
        "    y.append(0)\n",
        "for i in range(2880):\n",
        "    X.append(clean_text(fake_data_2['title'][i]))\n",
        "    y.append(0)\n",
        "for i in range(3450):\n",
        "    X.append(clean_text(true_data['title'][i]))\n",
        "    y.append(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GCc_Fszm6Z9"
      },
      "source": [
        "#Import dataset 6 \n",
        "data = pd.read_csv(\"/content/gdrive/MyDrive/data.csv\")\n",
        "X = []\n",
        "y = data['Label']\n",
        "for i in range(len(data)):\n",
        "    X.append(clean_text(data['Headline'][i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbcE6sc_rLbb"
      },
      "source": [
        "#Import dataset 7 \n",
        "X = []\n",
        "y = []\n",
        "\n",
        "#Get 2000 samples from dataset 6 \n",
        "data = pd.read_csv(\"/content/gdrive/MyDrive/data.csv\")\n",
        "for i in range(2000):\n",
        "    X.append(clean_text(data['Headline'][i]))\n",
        "    y.append(data['Label'][i])\n",
        "\n",
        "#Get all from dataset 5\n",
        "with open(\"/content/gdrive/MyDrive/special_fake_news.txt\",'r',encoding = 'latin-1') as file:\n",
        "    texts = file.read().split('\\n')\n",
        "    for text in texts:\n",
        "        if text:\n",
        "            X.append(clean_text(text))\n",
        "            y.append(0)\n",
        "\n",
        "with open(\"/content/gdrive/MyDrive/special_real_news.txt\",'r',encoding = 'latin-1') as file:\n",
        "    texts = file.read().split('\\n')\n",
        "    for text in texts:\n",
        "        if text:\n",
        "            X.append(clean_text(text))\n",
        "            y.append(1)\n",
        "\n",
        "#Get 2000 samples from dataset 1\n",
        "data = pd.read_csv(\"/content/gdrive/MyDrive/train.csv\")\n",
        "for i in range(5000):\n",
        "  X.append(clean_text(data['title'][i]))\n",
        "  y.append(data['label'][i])\n",
        "\n",
        "#Get 3000 samples from dataset 3\n",
        "fake_data = pd.read_csv(\"/content/gdrive/MyDrive/Fake.csv\",engine = 'python')\n",
        "true_data = pd.read_csv(\"/content/gdrive/MyDrive/True.csv\",engine = 'python')\n",
        "\n",
        "#Take 1100 samples from true data , 1900 samples from fake data\n",
        "for i in range(1100):\n",
        "    X.append(clean_text(fake_data['title'][i]))\n",
        "    y.append(0)\n",
        "for i in range(1900):\n",
        "    X.append(clean_text(true_data['title'][i]))\n",
        "    y.append(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpXohntep0bd"
      },
      "source": [
        "#Split test set\n",
        "from sklearn.model_selection import train_test_split \n",
        "X_train , X_test , y_train , y_test = train_test_split(X,y, test_size = 0.30,random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Irh3HMhp1X10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34ce94e5-80b2-4b44-afa3-458857dd0260"
      },
      "source": [
        "pip install textstat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting textstat\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/42/3e3691ff23d2f3e30ef18bd382d9450e992e2da7e01ca33d392b473eba05/textstat-0.7.1-py3-none-any.whl (99kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 4.3MB/s \n",
            "\u001b[?25hCollecting pyphen\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/5a/5bc036e01389bc6a6667a932bac3e388de6e7fa5777a6ff50e652f60ec79/Pyphen-0.10.0-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 30.0MB/s \n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.10.0 textstat-0.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3dWhEuOpiyK"
      },
      "source": [
        "#Use multinomial NB to fit model \n",
        "from sklearn.base import TransformerMixin\n",
        "class DenseTransformer(TransformerMixin):\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None, **fit_params):\n",
        "        return X.todense()\n",
        "\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler,MaxAbsScaler\n",
        "classifier = Pipeline([\n",
        "    ('features', FeatureUnion([\n",
        "        ('count_vectorizer', Pipeline([\n",
        "            ('vectorizer', CountVectorizer()),\n",
        "        ])),\n",
        "        ('more', Pipeline([\n",
        "            ('linguistic_features', FunctionTransformer(feature_selection_texts, validate=False)),\n",
        "        ]))\n",
        "    ])),\n",
        "    ('to_dense',DenseTransformer()),\n",
        "    ('scale',MinMaxScaler()),\n",
        "    ('nb',VotingClassifier(estimators = [('naive_bayes',MultinomialNB()),\n",
        "                                            ('logistic_regression',LogisticRegression(n_jobs = -1 , random_state = 0)),\n",
        "                                            #('knn',KNeighborsClassifier(n_neighbors=10,n_jobs = -1))\n",
        "                                            #('svm',SVC(random_state = 0))\n",
        "                                            ],\n",
        "                                            voting = 'hard',\n",
        "                                            n_jobs = -1) )])\n",
        "\n",
        "classifier = classifier.fit(X_train , y_train)\n",
        "y_pred = classifier.predict(X_test) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLIN7-2AqITF"
      },
      "source": [
        "#Making confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test , y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2jG79MoqLJu"
      },
      "source": [
        "def calculate_accuracy(cm):\n",
        "    tn = cm[0][0]\n",
        "    tp = cm[1][1]\n",
        "    fp = cm[0][1]\n",
        "    fn = cm[1][0]\n",
        "        \n",
        "    accuracy = (tn + tp) / (tn + tp + fp + fn)\n",
        "    precision = tp/(tp+fp)\n",
        "    recall = tp/(tp+fn)\n",
        "    f1 = 2*(precision*recall)/(precision + recall)\n",
        "    average = (accuracy + precision + recall + f1) /4 \n",
        "    return [accuracy , precision , recall , f1 , average]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2AiMi_aqMU1"
      },
      "source": [
        "acc = calculate_accuracy(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzepzKFOqNk9",
        "outputId": "6f6ee281-1f45-4f41-96ee-bb4f85edb50b"
      },
      "source": [
        "acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.814680710994075,\n",
              " 0.8635714285714285,\n",
              " 0.7647058823529411,\n",
              " 0.8111372022811137,\n",
              " 0.8135238060498896]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5XXLPXTm_kK",
        "outputId": "023f908e-9c29-429e-f693-fb678ca225bc"
      },
      "source": [
        "len(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10125"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    }
  ]
}